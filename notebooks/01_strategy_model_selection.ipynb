{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e044a84",
   "metadata": {},
   "source": [
    "\n",
    "# Spam Detection: Strategy, Models, and Selection (Do Not Execute Here)\n",
    "\n",
    "> **Important:** This notebook is provided as **code for your boss to run**. Please do **not** execute it in this environment.\n",
    "> It demonstrates multiple encoders and models, performs proper model selection, and **saves the best model** and associated\n",
    "> artifacts into the `models/` directory.\n",
    "\n",
    "## What this notebook does\n",
    "1. Loads a labelled spam dataset (default: SMS Spam Collection).  \n",
    "2. Tries multiple **text encoders** (TF‚ÄìIDF word & char n-grams, HashingVectorizer, SentenceTransformer embeddings ‚Äî optional).  \n",
    "3. Trains multiple **models** (Logistic Regression, Linear SVM, Multinomial Naive Bayes, RandomForest; optional: LinearSVC calibrated).  \n",
    "4. Uses **pipelines** + **cross-validation** and **RandomizedSearchCV** for robust selection.  \n",
    "5. Evaluates with **ROC-AUC** and **PR-AUC**, plus confusion matrix and classification report.  \n",
    "6. Saves the **best pipeline** (vectorizer + model) to `models/best_model.joblib` and metadata to `models/metadata.json`.\n",
    "\n",
    "## Dataset\n",
    "- Default: SMS Spam Collection dataset (labelled \"ham\"/\"spam\").\n",
    "- You can replace this with your own CSV that has `text` and `label` columns.\n",
    "\n",
    "## Encoders & Models Included\n",
    "**Encoders**\n",
    "- `TfidfVectorizer` (word-level, char-level, mixed n-grams)\n",
    "- `HashingVectorizer` (for speed/memory; paired with an online learner or standard linear models)\n",
    "- *(Optional)* Sentence embeddings via `sentence-transformers` (commented out by default to keep dependencies light)\n",
    "\n",
    "**Models**\n",
    "- Logistic Regression (liblinear/saga)\n",
    "- Linear SVM (LinearSVC with CalibratedClassifierCV for probabilities)\n",
    "- Multinomial Naive Bayes\n",
    "- RandomForest (as a non-linear baseline; expect slower training on large data)\n",
    "\n",
    "## Metrics\n",
    "We prioritize **ROC-AUC** and **Average Precision (PR-AUC)**, track **F1**, and also log confusion matrix.\n",
    "\n",
    "## Reproducibility\n",
    "- Set `RANDOM_STATE` where applicable.\n",
    "- Log all hyperparameters and CV splits.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0828486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Data Loading (Read from local filename in this notebook's folder)\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "CSV_PATH = Path(\"Spam_Detector.csv\")  # Expect the file to be next to this notebook\n",
    "assert CSV_PATH.exists(), f\"‚ùå File not found: {CSV_PATH}. Place Spam_Detector.csv alongside this notebook.\"\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "required_cols = {\"text\", \"label\"}\n",
    "assert required_cols.issubset(df.columns), f\"‚ùå CSV must contain the columns: {required_cols}\"\n",
    "\n",
    "# Normalize string labels if present\n",
    "if df[\"label\"].dtype == object:\n",
    "    df[\"label\"] = df[\"label\"].astype(str).str.lower().map({\"spam\": 1, \"ham\": 0}).fillna(df[\"label\"]).astype(int)\n",
    "\n",
    "print(\"‚úÖ Loaded\", len(df), \"rows from\", CSV_PATH)\n",
    "print(df[\"label\"].value_counts(normalize=True).rename(\"class_proportion\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209f99af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚öôÔ∏è Setup (Do Not Execute Here)\n",
    "# This cell installs imports if needed. Your boss should run this in their environment (local or Colab).\n",
    "# !pip install -U scikit-learn pandas numpy joblib matplotlib seaborn tqdm\n",
    "\n",
    "import os, json, math, random, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import Bunch\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40ba3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚úÇÔ∏è Train/Validation Split (Do Not Execute Here)\n",
    "X = df[\"text\"].astype(str).values\n",
    "y = df[\"label\"].values\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "print(len(X_train), len(X_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc4c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üß™ Define Pipelines & Hyperparameters (Do Not Execute Here)\n",
    "# We'll configure a few candidate pipelines and hyperparameter grids for RandomizedSearchCV.\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "candidates = []\n",
    "\n",
    "# 1) TF-IDF + Logistic Regression\n",
    "pipe_lr = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(strip_accents='unicode')),\n",
    "    (\"clf\", LogisticRegression(max_iter=2000, n_jobs=None, random_state=RANDOM_STATE, solver=\"liblinear\"))\n",
    "])\n",
    "params_lr = {\n",
    "    \"tfidf__ngram_range\": [(1,1), (1,2)],\n",
    "    \"tfidf__min_df\": [1,2,3,5],\n",
    "    \"tfidf__max_df\": [0.9, 1.0],\n",
    "    \"tfidf__sublinear_tf\": [True, False],\n",
    "    \"clf__C\": np.logspace(-2,2,10),\n",
    "    \"clf__penalty\": [\"l1\",\"l2\"]\n",
    "}\n",
    "candidates.append((\"tfidf+logreg\", pipe_lr, params_lr))\n",
    "\n",
    "# 2) TF-IDF (char + word) + LinearSVC (Calibrated for probabilities)\n",
    "pipe_svc = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(strip_accents='unicode')),\n",
    "    (\"svc\", CalibratedClassifierCV(LinearSVC(random_state=RANDOM_STATE), cv=3))\n",
    "])\n",
    "params_svc = {\n",
    "    \"tfidf__ngram_range\": [(1,2), (1,3)],\n",
    "    \"tfidf__analyzer\": [\"word\", \"char\", \"char_wb\"],\n",
    "    \"tfidf__min_df\": [1,2,3],\n",
    "    \"tfidf__max_df\": [0.9, 1.0]\n",
    "}\n",
    "candidates.append((\"tfidf+svc(calibrated)\", pipe_svc, params_svc))\n",
    "\n",
    "# 3) TF-IDF + MultinomialNB\n",
    "pipe_mnb = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(strip_accents='unicode')),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "params_mnb = {\n",
    "    \"tfidf__ngram_range\": [(1,1), (1,2)],\n",
    "    \"tfidf__min_df\": [1,2,3],\n",
    "    \"tfidf__max_df\": [0.9, 1.0],\n",
    "    \"clf__alpha\": np.logspace(-3,0,8)\n",
    "}\n",
    "candidates.append((\"tfidf+mnb\", pipe_mnb, params_mnb))\n",
    "\n",
    "# 4) HashingVectorizer + Logistic Regression\n",
    "pipe_hash_lr = Pipeline([\n",
    "    (\"hash\", HashingVectorizer(alternate_sign=False)),\n",
    "    (\"clf\", LogisticRegression(max_iter=2000, solver=\"liblinear\", random_state=RANDOM_STATE))\n",
    "])\n",
    "params_hash_lr = {\n",
    "    \"hash__n_features\": [2**16, 2**18, 2**20],\n",
    "    \"clf__C\": np.logspace(-2,2,10),\n",
    "    \"clf__penalty\": [\"l1\",\"l2\"]\n",
    "}\n",
    "candidates.append((\"hash+logreg\", pipe_hash_lr, params_hash_lr))\n",
    "\n",
    "# 5) TF-IDF + RandomForest (baseline non-linear)\n",
    "pipe_rf = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(strip_accents='unicode', ngram_range=(1,2))),\n",
    "    (\"rf\", RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=300))\n",
    "])\n",
    "params_rf = {\n",
    "    \"tfidf__min_df\": [1,3,5],\n",
    "    \"tfidf__max_df\": [0.9, 1.0],\n",
    "    \"rf__max_depth\": [None, 10, 20],\n",
    "    \"rf__min_samples_split\": [2, 5, 10]\n",
    "}\n",
    "candidates.append((\"tfidf+rf\", pipe_rf, params_rf))\n",
    "\n",
    "# (Optional) SentenceTransformers + Linear models could be added here for larger budgets.\n",
    "print(f\"Configured {len(candidates)} candidate pipelines.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f3e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîç Randomized Search over Candidates (Do Not Execute Here)\n",
    "# We evaluate each candidate with RandomizedSearchCV and keep the best overall by PR-AUC primarily, then ROC-AUC.\n",
    "\n",
    "def eval_candidate(name, pipe, param_dist, X_train, y_train, X_valid, y_valid, n_iter=20):\n",
    "    search = RandomizedSearchCV(\n",
    "        pipe,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_iter,\n",
    "        scoring=\"average_precision\",\n",
    "        n_jobs=-1,\n",
    "        cv=cv,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=1\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    best = search.best_estimator_\n",
    "    y_proba = best.predict_proba(X_valid)[:, 1] if hasattr(best, \"predict_proba\") else None\n",
    "    y_pred = best.predict(X_valid)\n",
    "    pr_auc = average_precision_score(y_valid, y_pred if y_proba is None else y_proba)\n",
    "    if y_proba is None:\n",
    "        # fall back to decision_function if available\n",
    "        if hasattr(best, \"decision_function\"):\n",
    "            scores = best.decision_function(X_valid)\n",
    "            try:\n",
    "                roc = roc_auc_score(y_valid, scores)\n",
    "            except:\n",
    "                roc = float(\"nan\")\n",
    "        else:\n",
    "            roc = roc_auc_score(y_valid, y_pred)\n",
    "    else:\n",
    "        roc = roc_auc_score(y_valid, y_proba)\n",
    "    report = classification_report(y_valid, y_pred, digits=4)\n",
    "    cm = confusion_matrix(y_valid, y_pred).tolist()\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"search\": search,\n",
    "        \"best\": best,\n",
    "        \"pr_auc\": float(pr_auc),\n",
    "        \"roc_auc\": float(roc),\n",
    "        \"report\": report,\n",
    "        \"cm\": cm,\n",
    "        \"best_params\": search.best_params_\n",
    "    }\n",
    "\n",
    "results = []\n",
    "for name, pipe, param_dist in candidates:\n",
    "    res = eval_candidate(name, pipe, param_dist, X_train, y_train, X_valid, y_valid, n_iter=20)\n",
    "    print(f\"\\n{name}: PR-AUC={res['pr_auc']:.4f} | ROC-AUC={res['roc_auc']:.4f}\")\n",
    "    print(res[\"report\"])\n",
    "    results.append(res)\n",
    "\n",
    "# Select best by PR-AUC, then ROC-AUC\n",
    "results = sorted(results, key=lambda r: (r[\"pr_auc\"], r[\"roc_auc\"]), reverse=True)\n",
    "best = results[0]\n",
    "print(\"Best model:\", best[\"name\"])\n",
    "print(\"Best params:\", best[\"best_params\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c962d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üíæ Save Best Model & Metadata (Do Not Execute Here)\n",
    "MODELS_DIR = Path(\"models\")\n",
    "MODELS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "best_path = MODELS_DIR / \"best_model.joblib\"\n",
    "joblib.dump(best[\"best\"], best_path)\n",
    "\n",
    "metadata = {\n",
    "    \"selected_model\": best[\"name\"],\n",
    "    \"best_params\": best[\"best_params\"],\n",
    "    \"pr_auc\": best[\"pr_auc\"],\n",
    "    \"roc_auc\": best[\"roc_auc\"],\n",
    "    \"random_state\": RANDOM_STATE\n",
    "}\n",
    "with open(MODELS_DIR / \"metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", best_path, \"and metadata.json\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
